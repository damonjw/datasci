{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: document models\n",
    "_[Original](http://mlg.eng.cam.ac.uk/teaching/4f13/1819/cw/coursework3.pdf) by Carl Rasmussen and Manon Kok for [CUED course 4f13](http://mlg.eng.cam.ac.uk/teaching/4f13/1819/). This version adapted by Damon Wischik._\n",
    "\n",
    "This coursework involves aggregating, summarizing, and joining datasets. This may be done with straight Python, or with MATLAB-style manipulations using `numpy`, or with `pandas` dataframes. If you anticipate future work in machine learning and data science then you should learn to use `pandas` dataframes, and you may find it helpful to follow the walkthrough in [Section 3](https://github.com/damonjw/scicomp/blob/master/notes3_pandas.ipynb) of IA _Scientific Computing_. If you prefer not to use dataframes, and you have questions about how they are being used in the code snippets below, ask your classmates or Dr Wischik.\n",
    "\n",
    "**What to submit.**\n",
    "Your answers should contain an explanation of what you do, and\n",
    "2&ndash;4 central commands to achieve it. \n",
    "The focus of your answer should be\n",
    "_interpretation:_ explain what the numerical values and graphs\n",
    "you produce _mean,_ and why they are as they are.  The text of\n",
    "your answer to each question should be no more than a paragraph or\n",
    "two. Marks will be awarded based on the clarity and insight in your explanations.\n",
    "\n",
    "DO NOT SUBMIT FULL SOURCE CODE, unless it is as an appendix. Do not repeat the question text in your answers. If you submit your answers as a Jupyter notebook, structure the notebook in two sections: a section at the top for the examiner to read with just your answers and trimmed code snippets, and a section at the bottom with all your working code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import pandas\n",
    "import requests, io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import\n",
    "\n",
    "The data is provided as `https://www.cl.cam.ac.uk/teaching/2122/DataSci/data/kos_doc_data.mat`. It contains two matrices $A$ and $B$ for training and testing respectively, both matrices with 3 columns: document ID, word ID, and word count. The words themselves are the vector $V$, where e.g. `V[840]='bush'`. The following snippet reads in the data, and converts $A$ and $B$ to dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.cl.cam.ac.uk/teaching/2122/DataSci/data/kos_doc_data.mat')\n",
    "with io.BytesIO(r.content) as f:\n",
    "    data = scipy.io.loadmat(f)\n",
    "    V = np.array([i[0] for i in data['V'].squeeze()])\n",
    "    A,B = [pandas.DataFrame({'doc_id': M[:,0]-1, 'word_id': M[:,1]-1, 'count': M[:,2]}, \n",
    "                            columns=['doc_id','word_id','count']) \n",
    "           for M in (data['A'],data['B'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (a): simple categorical model\n",
    "\n",
    "Suppose we model words in a document as independent samples from a categorical distribution with parameter $\\beta$, where $\\beta_v$ is the probability of word $v\\in V$. Using $A$ as the training set, find the maximum likelihood estimator $\\hat{\\beta}$, and plot the 20 most-probable words in a histogram. What is the log probability of the test document `doc_id=2527`, given $\\hat{\\beta}$? Briefly interpret your answer.\n",
    "\n",
    "Note: you can plot a histogram with\n",
    "```python\n",
    "fig,ax = plt.subplots(figsize=(5,8))\n",
    "ax.barh(np.arange(20), ???, align='center')\n",
    "ax.set_yticks(np.arange(20))\n",
    "ax.set_yticklabels(???)\n",
    "ax.set_xlabel(r'$\\hat{\\beta}$')\n",
    "ax.invert_yaxis()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (b): Bayesian inference\n",
    "\n",
    "For the categorical model in part (a), use Bayesian inference to find the posterior distribution of $\\beta$ given the training set $A$, using a symmetric Dirichlet distribution with concentration parameter $\\alpha=0.1$ as prior. Let $\\tilde{\\beta}_v$ be the posterior predictive probability of word $v\\in V$, i.e. the posterior probability that a newly chosen word is $v$. Derive an expression for $\\tilde{\\beta}_v$, and compare it to $\\hat{\\beta}_v$. Explain the implications, both for common and for rare words.\n",
    "\n",
    "Hint: $\\Gamma(z+1)=z\\,\\Gamma(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (c): interpretation\n",
    "\n",
    "The log likelihood $\\log p(w)$ of a document $w$ depends on the number of words in the document, and it's more useful to report the log likelihood per word, $n^{-1}\\log p(w)$ where $n$ is the number of words in $w$. \n",
    "\n",
    "(In information theory, $-n^{-1}\\log_2 p(w)$ can be interpreted as the number of bits per word needed to encode or transmit $w$. In text modelling, it is more common to report _per-word perplexity_ $p(w)^{-1/n}$.)\n",
    "\n",
    "For the trained Bayesian model from part (b), what is the per-word log likelihood of the test document `doc_id=2000`? Plot a histogram showing the distribution of per-word log likelihood over all the test documents (using [`plt.hist`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html?highlight=matplotlib%20pyplot%20hist#matplotlib.pyplot.hist)). Pick out two documents, one with high per-word perplexity and one with low per-word perplexity, show their contents, and interpret the difference between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (d): Gibbs sampler for the mixture-of-multinomials model\n",
    "\n",
    "The Bayesian mixture-of-multinomials model can be described by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['critic' 'indymedia' 'susan' 'citizenship' 'cycles']\n",
      "['cool' 'celebrity']\n",
      "['jennings' 'quarter' 'token' 'governance']\n"
     ]
    }
   ],
   "source": [
    "def bmm_generate(doc_length, V, α, γ, K):\n",
    "    # doc_length = [num words in doc1, num words in doc2, ...]\n",
    "    θ = np.random.dirichlet(α * np.ones(K))              # prob dist over document classes {1,...,K}\n",
    "    β = np.random.dirichlet(γ * np.ones(len(V)), size=K) # for each doc class, a prob dist over words\n",
    "    z = np.random.choice(K, p=θ, size=len(doc_length))   # doc class of each document\n",
    "    return [np.random.choice(V, p=β[zd], size=nd) for zd,nd in zip(z, doc_length)]\n",
    "\n",
    "for doc in bmm_generate(doc_length=[5,2,4], V=V, α=10, γ=.1, K=20):\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements a collapsed Gibbs sampler. Complete the line that defines `logp`. In each sweep, the Gibbs sampler produces a sample of document classes, and this sample induces a posterior predictive distribution for the probability of each class. Plot how this distribution evolves as a function of the number of Gibbs sweeps. How many iterations does it take to converge?\n",
    "```\n",
    "def bmm_gibbs(doc_label, word_id, count, W, α, γ, K):\n",
    "    # doc_labels = distinct values of doc_label\n",
    "    # doc_index = a list as long as doc_label\n",
    "    #             such that doc_labels[doc_index[j]] = doc_label[j]\n",
    "    doc_labels, doc_index = np.unique(doc_label, return_inverse=True)\n",
    "\n",
    "    # z[i] = class of document i, where i enumerates the distinct doc_labels\n",
    "    # doc_count[k] = number of documents of class k\n",
    "    z = np.random.choice(K, len(doc_labels))\n",
    "    doc_count = np.zeros(K, dtype=int)\n",
    "    for k in z: doc_count[k] += 1\n",
    "\n",
    "    # occurrences[k,w] = number of occurrences of word_id w in documents of class k\n",
    "    # word_count[k] = total number of words in documents of class k\n",
    "    x = pandas.DataFrame({'doc_class': z[doc_index], 'word_id': word_id, 'count': count}) \\\n",
    "        .groupby(['doc_class', 'word_id']) \\\n",
    "        ['count'].apply(sum) \\\n",
    "        .unstack(fill_value=0)\n",
    "    occurrences = np.zeros((K, len(V)))\n",
    "    occurrences[x.index.values.reshape((-1,1)), x.columns.values] = x\n",
    "    word_count = np.sum(occurrences, axis=1)\n",
    "    \n",
    "    while True:\n",
    "        for i in range(len(doc_labels)):\n",
    "\n",
    "            # get the words,counts for document i\n",
    "            # and remove this document from the counts\n",
    "            w,c = word_id[doc_index==i].values, count[doc_index==i].values\n",
    "            occurrences[z[i], w] -= c\n",
    "            word_count[z[i]] -= sum(c)\n",
    "            doc_count[z[i]] -= 1\n",
    "\n",
    "            # Find the log probability that this document belongs to class k, marginalized over θ and β\n",
    "            logp = [... for k in range(K)]\n",
    "            p = np.exp(logp - np.max(logp))\n",
    "            p = p/sum(p)\n",
    "\n",
    "            # Assign this document to a new class, chosen randomly, and add back the counts\n",
    "            k = np.random.choice(K, p=p)\n",
    "            z[i] = k\n",
    "            occurrences[k, w] += c\n",
    "            word_count[k] += sum(c)\n",
    "            doc_count[k] += 1\n",
    "        \n",
    "        yield np.copy(z)\n",
    "```\n",
    "The Gibbs sampler may be run as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = bmm_gibbs(A['doc_id'], A['word_id'], A['count'], W=len(V), α=10, γ=.1, K=20)\n",
    "NUM_ITERATIONS = 20\n",
    "res = np.stack([next(g) for _ in range(NUM_ITERATIONS)])\n",
    "# this produces a matrix with one row per iteration and a column for each unique doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (e): interpretation\n",
    "\n",
    "Let $\\alpha=10$, $\\gamma=0.1$, $K=20$. Run the Gibbs sampler until it converges, and find the posterior predictive probabilities for topics, and for words within each topic. \n",
    "For each the 8 most popular topics, print the probability of the topic and the 8 most probable words and their probabilities.\n",
    "Display probabilities in _shannons_, i.e. display a probability $p$ as $-\\log_2 p$. An increase of 1 shannon corresponds to a 50% decrease in probability.\n",
    "\n",
    "Rerun with different random seeds. Do you think this method has succeeded in identifying topics?\n",
    "\n",
    "There are some words that are very common across all topics. Find the _distinctive_ words for each topic. _[This is open-ended, and it's up to you to invent your own answer. Don't overthing it, and don't write more than a paragraph justifying your choice.]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (f): evaluation\n",
    "\n",
    "Give a formula for per-word log likelihood for the mixture model, in terms of the posterior predictive probabilities for topics and words.\n",
    "\n",
    "Plot a histogram showing the distribution of per-word log likelihood over all the test documents for the model in part (e). Also plot the histogram obtained from $K=8$, and the histogram from the plain multinomial model in part (c). Which model do you prefer, and why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
